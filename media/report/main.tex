\documentclass{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{xfrac}
\usepackage{siunitx}                    % for scientific notation \num{}
\usepackage{authblk}                    % for authors
\usepackage{tikz}                       % for circled {}
\usepackage{systeme}                    % for system of equations bracket
\usepackage{verbatim}                   % for comment
\usepackage{xfp}                        % for floating point operations in macros
\usepackage{graphicx}                   % for cropping the images
\usepackage{dsfont}                     % for doublestroke fonts
\usepackage{float}
\usepackage{hyperref}                   % hyperlink
\usepackage[english]{babel}
\usepackage[square, numbers]{natbib}
\usepackage[nottoc]{tocbibind}          % Includes "References" in the table of contents

\bibliographystyle{abbrvnat}

\hypersetup{
	colorlinks=true, 
	citecolor=blue,
}

\AtBeginDocument{\renewcommand{\bibname}{References}}

\title{Video Prediction and Learning Causality}
\subtitle{Independent Work Report (MAE 340, Spring 2021)}
\author{Matthew Coleman}
\date{April 26, 2022}

\begin{document}

\maketitle

\vspace{8cm}
\Large
\textit{This project represents my own work, in accordance with the University regulations.} \\
\hspace*{\fill} \large /s/Matthew Coleman
\normalsize

\newpage
\tableofcontents
\newpage

\section{Introduction}
\label{sec:intro}

While humans cannot perfectly predict the future, they are indeed capable of
inferring a great deal of information about near events in the future, and this
knowledge greatly aids them in planning out their actions, such as which
movements to take to reach a goal. This ability to forecast the future is a
direct result of an understanding of causality that is learned through
observation and interaction \cite{human_learning_sequences}.

A great amount of human predictions are, of course, erroneous in major
respects, but even the humans least adept at inferring far-off outcomes and
consequences still are masters of learning very near-term ones. For example,
humans have a good sense for where a car will move in the street, or which
direction a pedestrian may continue walking. Even a young child can predict
where to toss a football to a moving receiver, and even this small knowledge
reveals an infinite wisdom compared to the most advanced video prediction
methods.

The task of video prediction is comprised of several open challenges in
computer vision; it uses some of the most recent model architectures that have
been developed and it even contends directly with an impossible task
altogether, which is to predict the future. Although it is a particularly
confusing task, it also has the potential for immense impact and immediate
practical applications, such as in autonomous driving \cite{eg_self_driving},
video interpolation \cite{eg_video_interp} and most interesting in the context
of this report, robotic control systems \cite{eg_robot_control}.

This project will examine the current state-of-the-art in video prediction
models, report on several experiments carried out by implementing and testing
such a model on various existing datasets, and attempt to make meaningful
conclusions about video prediction and learning causality.

\section{The Task of Video Prediction}
\label{sec:task}

\newcommand{\Xseq}{$\boldsymbol{X} = \left( X_1 , X_2 , X_3 \cdots X_n \right)$}
\newcommand{\Yseq}{$\boldsymbol{Y} = \left( Y_1 , Y_2 , Y_3 \cdots Y_m \right)$}
\newcommand{\Yhatseq}{
	$\hat{\boldsymbol{Y}} = 
	\left( \hat{Y}_1 , \hat{Y}_2 , \hat{Y}_3 \cdots \hat{Y}_n \right)$
}

The task of video prediction is to construct an approximation for the
completion of a sequence of frames, given only the initial sequence of frames.
Formally, given an ordered set of $n$ image frames \Xseq, the task is to
predict the latter $m$ frames of the sequence \Yseq, each frame of which having
the same dimensions, for example with $c$ channels, height $h$, and width $w$.
At each inference in training, the model predictions \Yhatseq are conditioned
on the input sequence $\boldsymbol{X}$, and the model weights are updated
typically by the gradient of a loss function computed between the predictions
and ground truth sequence $\boldsymbol{Y}$ directly. Critically, since there is
no human intervention or labeling required for the model to do this, video
prediction is a self-supervised task \cite{video_prediction_survey}.

\newpage
\section{Families of Prediction Models}
\label{sec:families}

Modern video prediction models tend to adopt several canonical architectures,
which are normally simple and easily generalizable for specific tasks, such that
they can be used as building blocks or blueprints within larger architectures.
For example, RNNs and generative networks are each successful and well-tested
paradigms used in many other machine learning domains outside of computer
vision, but they are especially utilized within video prediction because of
their key properties and strengths, particularly of RNNs to work on
time-sequential data and of generative networks to ``imagine'' new data
within a distribution. These paradigms are used extensively in Convolutional
LSTM, FutureGAN \cite{futuregan}, and SAVP models \cite{savp}. A short
description of each family and its relevance to video prediction is given
below:

\subsection{Recurrent Models}
\label{subsec:recurrent}

Recurrent neural networks (RNNs) consist of a network of nodes that stretches
over some sequential information, typically in the form of a time sequence
(Which is the case in video prediction). Each node will perform another
learning technique on the data in sequence (This could be a convolution
operation, a linear layer, or a combination of several, for example) and output
its own activation. Commonly, this is implemented not with a network of
individual nodes but rather in the form of a feedback loop over a single node,
which passes some data embedding forward through the network to itself in a
loop (a hidden state, or variable), only taking in new data from the original
input sequence at each step. In this way, an RNN equipped with convolution is
capable of learning from time-varying information while preserving
spatio-temporal relations (That is, relationships in the data that exist over
space, such as the shape of a person's leg and hip, as well as relationships in
the data that exist over time, such as the motion of a person walking, will be
preserved in the final activations of the network).

The outputs of each node are then used for other purposes, depending on the
task, and the result is then backpropagated against a loss function. In machine
learning terminology, this is referred to as backpropagation through time
(BPTT) \cite{rnn_and_lstm_fundamentals}, since a gradient must be computed in
the input sequence's reverse order, i.e., backwards through time, and in the
case of an RNN implemented with feedback, this gradient must be computed with
respect to the input data at each time step and then added to the weights of
the single node in sum.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{figures/rnn_arch.png}
	\end{center}
	\caption{General RNN Architecture}
	\label{fig:rnn_arch}
\end{figure}

\subsection{Generative Models}
\label{subsec:generative}

Generative neural networks (for example, GANs) consist of mostly the same
architecture as discriminative neural nets, such as the ones which are
classically used for image classification. While discriminative nets seek to
learn the conditional probability $p(y \mid x)$ of an input $x$ belonging to a
particular class $y$, generative nets seek to learn the conditional probability
distribution $p(x \mid y)$ of an input data given the output, allowing them to
make inferences in the form of ``imagined'' data that might belong to the same
distribution as $x$ \cite{gan_original}. In short, discriminative models would
look at many Van Gogh paintings and fakes in order to learn to differentiate
between them, and generative models would look at many Van Gogh paintings in
order to learn how to paint like Van Gogh.

In practice, this is done by passing a low-dimensional embedding vector from a
known latent distribution (such as the kind that may come as an activation from
a discriminative net) through a typical network in reverse, generating an
upscaled output in the same shape as the targeted learning data.

\section{Video Prediction Models}
\label{sec:families}

\subsection{Convolutional LSTM}
\label{subsec:conv_lstm}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{figures/lstmcell_arch.png}
	\end{center}
	\caption{LSTM Cell Architecture}
	\label{fig:rnn_arch}
\end{figure}

\subsection{FutureGAN}
\label{subsec:futuregan}

\subsection{SAVP}
\label{subsec:savp}

\newpage
\section{Datasets}
\label{sec:datasets}

\subsection{MovingMNIST}
\label{subsec:mmnist}

\subsection{KTH}
\label{subsec:kth}

\subsection{BAIR}
\label{subsec:bair}

\newpage
\section{Experiments}
\label{sec:experiments}

\subsection{Sequence Prediction}
\label{subsec:experiment_sp}

\subsection{Video Prediction}
\label{subsec:experiment_vp}

\newpage
\section{Conclusion}
\label{sec:conclusion}

% ------------------------------------------------------------------------------
% References 
% ------------------------------------------------------------------------------

\newpage
\bibliography{main}
\newpage

\end{document}

