\documentclass{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{xfrac}
\usepackage{siunitx}                                % for scientific notation \num{}
\usepackage{authblk}                                % for authors
\usepackage{tikz}                                   % for circled {}
\usepackage{systeme}                                % for system of equations bracket
\usepackage{verbatim}                               % for comment
\usepackage{xfp}                                    % for floating point operations in macros
\usepackage{graphicx}                               % for cropping the images
\usepackage{dsfont}                                 % for doublestroke fonts
\usepackage{float}
\usepackage{hyperref}                               % hyperlink
\usepackage[english]{babel}
\usepackage[square, numbers]{natbib}
\usepackage[nottoc, notlof, notlot]{tocbibind}     % Includes "References" in the table of contents
\usepackage{caption}
\usepackage{listings}
\usepackage{xparse}
\usepackage[none]{hyphenat}

\bibliographystyle{abbrvnat}
% \captionsetup{justification=centering}
\lstset{language=python, keywordstyle={\bfseries \color{blue}}}

\NewDocumentCommand{\codeword}{v}{\texttt{\textcolor{blue}{#1}}}

\hypersetup{
	colorlinks=true, 
	citecolor=blue,
}

\AtBeginDocument{\renewcommand{\bibname}{References}}

\title{Video Prediction}
\subtitle{Independent Work Report (MAE 340, Spring 2021) \\ Advisor: Professor Olga Russakovsky}
\author{Matthew Coleman}
\date{April 26, 2022}

\begin{document}

\maketitle

\vspace{8cm}
\begin{center}
	\Large \textit{This project represents my own work, \\ in accordance with the University regulations.} \\
	\vspace{0.2cm}
	\large /s/Matthew Coleman
\end{center}
\normalsize

\newpage
\tableofcontents
\newpage

\section{Introduction}
\label{sec:intro}

While humans cannot perfectly predict the future, they are indeed capable of
inferring a great deal of information about near events in the future, and this
knowledge greatly aids them in planning out their actions, such as which
movements to take to reach a goal. This ability to forecast the future is a
direct result of an understanding of causality that is learned through
observation and interaction \cite{human_learning_sequences}.

A great amount of human predictions are erroneous in major respects, but even
the humans least adept at inferring far-off outcomes and consequences still are
masters of learning very near-term ones. For example, humans have a good sense
for where a car will move in the street, or which direction a pedestrian may
continue walking. Even a young child can predict where to toss a football to a
moving receiver, and even this small knowledge reveals an infinite wisdom
compared to the most advanced video prediction methods.

The task of video prediction is comprised of several open challenges in
computer vision; it uses some of the most recent model architectures that have
been developed and it even contends directly with an impossible task
altogether, which is to predict the future. Although it is a particularly
confusing task, it also has the potential for immense impact and immediate
practical applications, such as in autonomous driving \cite{eg_self_driving},
video interpolation \cite{eg_video_interp} and most interesting in the context
of this report, robotic control systems \cite{eg_robot_control}.

This project will examine the current state-of-the-art in video prediction
machine learning models, report on several experiments carried out by
implementing and testing such a model on various existing datasets, and attempt
to make meaningful conclusions about video prediction and learning causality.

\section{The Task of Video Prediction}
\label{sec:task}

\newcommand{\Xseq}{$\boldsymbol{X} = \left( X_1 , X_2 , X_3 \cdots X_n \right)$}
\newcommand{\Yseq}{$\boldsymbol{Y} = \left( Y_1 , Y_2 , Y_3 \cdots Y_m \right)$}
\newcommand{\Yhatseq}{
	$\hat{\boldsymbol{Y}} = 
	\left( \hat{Y}_1 , \hat{Y}_2 , \hat{Y}_3 \cdots \hat{Y}_n \right)$
}

The task of video prediction is to construct an approximation for the
completion of a sequence of frames, given only the initial sequence of frames.
Formally, given an ordered set of $n$ image frames \Xseq, the task is to
predict the latter $m$ frames of the sequence \Yseq, each frame of which having
the same dimensions, for example with $c$ channels, height $h$, and width $w$.
At each inference in training, the model predictions \Yhatseq, with the same
dimensions as the inputs, are conditioned on the input sequence
$\boldsymbol{X}$, and the model weights are updated typically by the gradient
of a loss function computed between the predictions and ground truth sequence
$\boldsymbol{Y}$ directly. Critically, since there is no human intervention or
labeling required for the model to do this, and models typically are able to
learn from the implicit temporal organiziation of the video data, video
prediction is a self-supervised task \cite{video_prediction_survey}.

% A great deal of discussion in video prediction research centers around exactly
% how these predictions are computed, specifically by testing different model
% architectures, loss functions, and training approaches, however, there is also
% a good amount of disagreement on how model predictions are actually evaluated.
% For example, models trained using pixel-wise loss functions such as MSE will
% typically prefer blurrier predictions in order to capture the underlying
% stochasticity of the data, whereas GANs will prefer to produce more realistic
% data, even if the predictions are further off the mark.

\newpage
\section{Families of Prediction Models}
\label{sec:families}

Modern video prediction models tend to adopt several canonical architectures,
which are normally simple and easily generalizable for specific tasks, such
that they can be used as building blocks or blueprints within larger
architectures. Understanding what each architecture seeks to do on a high level
is imperative to understanding what kind of output one should expect from the
model, since they are each very unique, and research implementations make use
of them in different ways.

For example, RNNs and generative networks are each successful and well-tested
paradigms used in many other machine learning domains outside of computer
vision, but they are especially utilized within video prediction because of
their key properties and strengths, particularly of RNNs to work on
time-sequential data and of generative networks to ``imagine'' new data within
a distribution. These paradigms are used extensively in Convolutional LSTM,
FutureGAN \cite{futuregan}, and SAVP models \cite{savp}, as well as many other
models in cutting-edge research. A short description of each family and its
relevance to video prediction is given below:

\subsection{Recurrent Neural Networks}
\label{subsec:rnn}

Formally, a Recurrent Neural Network (RNN) is the end-result of a mathematical
analysis of a nonlinear first-order non-homogeneous ordinary differential
equation describing the evolution of a state signal $s$ as a function of time,
along with an input signal $x$. The canonical statement of an RNN is given
below in the form of a discrete Delay Differential Equation (DDE)
\cite{rnn_and_lstm_fundamentals}:

\begin{equation}
	\begin{split}
		s_t & = W_s s_{t - 1} + W_r r_{t - 1} + W_x x_t + \theta_s \\
		r_t & = G ( s_t )
	\end{split}
	\label{eq:rnn_canonical}
\end{equation}

With $W_s$, $W_r$, and $W_x$ as weights which are either multiplied or
convolved with their respective signals, and $\theta_s$ as a bias term
which is typically added in element-wise fashion to $s$. This equation
also includes the read-out or output signal $r$, which is the activation
$G(z)$ of the state signal, and can be seen as the ``output'' of the network.
All together, this equation describes all of the moving parts of an RNN.

As a toy example (and ignoring some technical tricks that would be required to
make this actually happen), a network of this type could be capable of
transcribing seqeunces of a lecture by evaluating discrete audio samples
recorded from the event and outputting the spoken words in text form
\cite{rnn_and_lstm_fundamentals}. Another extremely common application of
vanilla RNNs is machine translation, in which the input sequence is text in one
language and the output is meant to be the translation of that text into
another language.

An easily understandable depiction of the RNN described in Equation
\ref{eq:rnn_canonical} can be found in Figure \ref{fig:rnn_arch}. This figure
also shows the ``unfolding'' or ``unrolling'' of the model, which is just a way
of seeing each time-step of the state signal $s_t$, input signal
$x_t$, and ouput signal $r_t$ (here replaced by $\hat{y}_t$,
denoting the network inferences) as they are produced over time.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{figures/rnn_arch.png}
	\end{center}
	\caption{RNN Architecture}
	\label{fig:rnn_arch}
\end{figure}

RNNs are generally trained using a techinque called Backpropagation Through
Time (BPTT), in which the gradient of a loss function taken at a certain
instance in time is used to update the model parameters recursively through the
history of the model, using the chain rule. In this way, RNN models trained
over long sequences have their parameters updated by the product of many
Jacobian matrices, which, just like a product of many real numbers, can vanish
or explode very easily \cite{rnn_training_challenges}. For this reason, along
with several others having to do with the long-term stability of RNNs, the Long
Short-Term Memory (LSTM) cell was developed with nonlinear, data-dependent
controls in the form of several ``gates'' that control input to and output from 
the cell's state \cite{rnn_and_lstm_fundamentals}.

\subsection{Long Short-Term Memory (LSTM) Cell}
\label{subsec:lstm}

The changes described in the following section take the form of a modified
system of equations \cite{rnn_review}: 

\newcommand{\csquig}{\tilde{c}}

\begin{equation}
	\begin{split}
		f_t           & = \sigma ( W_{fh} h_{t - 1} + W_{fx} x_t + b_f ) \\
		i_t           & = \sigma ( W_{ih} h_{t - 1} + W_{ix} x_t + b_i ) \\
		\csquig_t     & = \tanh ( W_{\csquig h} h_{t - 1} + W_{\csquig x} x_t + b_{\csquig} ) \\
		o_t           & = \sigma ( W_{oh} h_{t - 1} + W_{ox} x_t + b_o ) \\
		c_t           & = f_t \circ c_{t - 1} + i_t \circ \tilde{c}_t \\
		h_t           & = o_t \circ \tanh ( c_t )
	\end{split}
	\label{eq:lstm_canonical}
\end{equation}

The key differences in the LSTM are the four gates denoted by $f$, $i$,
$\tilde{c}$, and $o$. Other than these, the cell state $c$ and output $h$ are
analogous to the original RNN definition. Now, the input and output gates are
able to ``learn'' how to add information to the cell state or take information
for inferences, and the forget gate is able to remove information from the cell
state entirely. To see how this is done, recall that the sigmoid funtion
$\sigma (z)$ has an output range of $(0, 1)$, meaning that element-wise
multiplication by embeddings activated by sigmoid can minimize and essentially
nullify the cell state. In the input gate, this same technique is applied for
$i$ onto $\tilde{c}$, and in the output gate it is applied by $o$ onto $c$,
meaning that $i$ decides what from $h_{t - 1}$ is allowed into the cell state
and $o$ decides what from $c_{t - 1}$ is allowed out of the cell state into the
inference. A diagram of these connections is shown in Figure
\ref{fig:lstmcell_arch}.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{figures/lstmcell_arch.png}
	\end{center}
	\caption{LSTM Cell Architecture}
	\label{fig:lstmcell_arch}
\end{figure}

In this way, LSTM cells are capable of mitigating some of the drawbacks of RNNs
such as vanishing and exploding gradients, and they are capable of learning how
to make good inferences for very different sequences. In other words, they are
better able to approximate both $y_{t_1}$ and $y_{t_2 \gg t_1}$ using the same
parameters.

Now that some of the general architectures and methodologies are laid out,
however, the question still remains of how these models actually predict the
continuation $\boldsymbol{Y}$ of a sequence of frames $\boldsymbol{X}$ with any
sort of accuracy. In order to do this, one final methodology must be examined,
namely Sequence to Sequence (Seq2Seq) learning, which will lead to the final
implementation of a Convolutional LSTM used for experimentation in this
project.

\subsection{Sequence To Sequence Learning}
\label{subsec:seq2seq}

Seq2Seq learning is only a slight modification to the original usage of RNNs,
but they represent an elegant solution to a classical shortcoming of most Deep
Neural Networks (DNNs), which is that, despite their flexibility, they can only
be applied to problems in which inputs and outputs can fit into fixed,
discretized, vectors, and therefore must be of a certain length or size.
Seq2Seq learning solves this problem for video prediction as well as many other
tasks by using 2 LSTMs: an encoder to read the sequence and learn to create an
embedding vector, and a decoder, which is passed the embedding vector and
learns to generate the predicted sequence \cite{seq2seq_original}. A diagram of
this procedure is shown in Figure \ref{fig:seq2seq_arch}.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{figures/seq2seq_arch.png}
	\end{center}
	\caption{Seq2Seq Architecture}
	\label{fig:seq2seq_arch}
\end{figure}

The diagram shows how the input sequence $x$ is fed into the encoder LSTM for 3
steps (in practice this can be any length), how the encoder generates an
embedding vector $h_4$ which is passed to the decoder as an input, and how the
decoder generates the predicted sequence by passing the predicted output
$\hat{y}_{t}$ back into itself at the next time step as an input, continuing until
all predicted frames are generated.

This diagram also shows two layered LSTM cells, which is another key
modification used in this project's implementation. Note that each layer has
it's own states $h$ and $c$, and at each time step, the cells pass their $h$
embedding up to the next cell as an input. Deep, multilayered LSTMs have been
shown to significantly outperform shallow LSTMs for language translation
\cite{seq2seq_original}, however, this project will also analyze the effect of
depth on LSTM inferences for video prediction. 

In sum, these are the main methods used in this report's implementation of an
LSTM model for video prediction, which can be found at this
\href{https://github.com/msc5/junior-iw}{GitHub repository}.

% \subsection{Generative Models}
% \label{subsec:generative}
%
% \begin{figure}[H]
% 	\begin{center}
% 		\includegraphics[width=1\textwidth]{figures/gan_arch.png}
% 	\end{center}
% 	\caption{General GAN Architecture}
% 	\label{fig:gan_arch}
% \end{figure}
%
% Generative neural networks (for example, GANs) consist of mostly the same
% architecture as discriminative neural nets, such as the ones which are
% classically used for image classification. While discriminative nets seek to
% learn the conditional probability $p(y \mid x)$ of an input $x$ belonging to a
% particular class $y$, generative nets seek to learn the conditional probability
% distribution $p(x \mid y)$ of an input data given the output, allowing them to
% make inferences in the form of ``imagined'' data that might belong to the same
% distribution as $x$ \cite{gan_original}. In short, discriminative models would
% look at many Van Gogh paintings and fakes in order to learn to differentiate
% between them, and generative models would look at many Van Gogh paintings in
% order to learn how to paint like Van Gogh.
%
% In practice, this is done by passing a low-dimensional embedding vector from a
% known latent distribution (such as the kind that may come as an activation from
% a discriminative net) through a typical network in reverse, generating an
% upscaled output in the same shape as the targeted learning data.

% \section{Video Prediction Models}
% \label{sec:families}

% \subsection{Convolutional LSTM}
% \label{subsec:conv_lstm}
%
% \subsection{FutureGAN}
% \label{subsec:futuregan}
%
% \begin{figure}[H]
% 	\begin{center}
% 		\includegraphics[width=1\textwidth]{figures/futuregan_arch.png}
% 	\end{center}
% 	\caption{FutureGAN Architecture}
% 	\label{fig:futuregan_arch}
% \end{figure}
%
% \subsection{SAVP}
% \label{subsec:savp}

% \newpage
% \section{Datasets}
% \label{sec:datasets}
%
% The datasets used in this project are 
% Datasets used in video prediction tasks have a wide range, and often include datasets made for other tasks.
%
% \subsection{MovingMNIST}
% \label{subsec:mmnist}
%
% \subsection{KTH}
% \label{subsec:kth}
%
% \subsection{BAIR}
% \label{subsec:bair}

\newpage 
\section{Implementation and Training Details}
\label{sec:details}

\section{Datasets}
\label{sec:details}

\newpage
\section{Experiments}
\label{sec:experiments}

% \begin{table}
% 	\caption{Summary of Main ConvLSTM Results}
% 	\label{tab:results_summary}
% 	\begin{center}
% 		\begin{tabular}{ l l l l }
% 			\textbf{Dataset} & \textbf{Number of layers} & \textbf{Number of Parameters} & \textbf{Test MSE Loss} \\
% 			MovingMNIST & \\
% 			KTH & \\
% 			BAIR
% 		\end{tabular}
% 	\end{center}
% \end{table}

The main experimental procedure carried out in this report is the training and
testing of the Convolutional LSTM model on the MovingMNIST, KTH, and BAIR
datasets, however, in order to more gain a more robust understanding of LSTM
features and limitations, as well as to debug the training mechanisms in a much
simpler setting, it was useful to first test a Linear LSTM model on
one-dimensional sequential data before moving fully to video prediction. Three
datasets of this type were implemented: a dataset consisting of generated sin
waves with random frequency and phase offset, a dataset consisting of NASDAQ
close price data from 8 tech companies, and a dataset consisting of randomly
generated points. The choice of these datasets were intended to each test the
model with a varying degree of stochasticity, the sin waves being the most
deterministic, or predictable, the random points being the most stochastic, or
random, and the stock price data hopefully being somewhere in between. Some
specific details about these datasets are described in the following sections.

When researching and implementing a prediction model of any sort, it is
important to consider that certain sequences are simply impossible to predict,
and that even if the model was ``perfect'' by human standards, it would still
fail to perform this task perfectly. The interesting question here is not
whether the model will be able to achieve a certain accuracy in predicting the
sequences but rather where or how exactly will it fail? And where it does fail,
which features of the model's architecture and methodology are to blame?

All these questions lead directly into one of the major concerns in video
prediction research, which is that the task is inherently hard to judge
\cite{video_prediction_survey}, particularly that pixel-wise loss functions,
such as the Mean Squared Error (MSE) function used to train this model, cause
models to prefer blurry results that average out multiple possibilities for the
next time step rather than a single, clearer image that could be very wrong
using pixel-wise metrics. Models trained in this way are not directly learning
to produce images but merely to appease the loss function, and these results
may not be as clear or useful to humans.

% This Linear LSTM was implemented with nearly the same exact architecture as the
% Convolutional LSTM, however with Linear layers in place of convolutions, and as
% a result, a 2D convolution layer as the final step as opposed to 3D. Each time
% it was trained on sequences of length 50, for a total duration of 10 epochs.

\subsubsection{GeneratedSins Dataset}
\label{subsubsec:generatedsins_intro}

This dataset was generated by simply plotting a sinusoid function:

\begin{equation}
	\boldsymbol{X} (t) = \boldsymbol{Y} (t) = \frac{1}{2} ( \sin (\alpha t + \beta) + 1)
	\label{eq:generated_sins}
\end{equation}

With random $\alpha$ and $\beta$. This results in a sin wave with a random
frequency and phase offset plotted within the range $[0, 1]$. Although this
dataset has random features, once the sin wave is detected and learned by the
model, it should be relatively easy to predict, since sin waves are perfectly
periodic. $\boldsymbol{X}$ is evaluated from $[0, 0.5)$, and $\boldsymbol{Y}$
is evaluated from $(0.5, 1]$. In sum, the train split consists of 22000
samples, and the test set consists of 7000. 

\subsubsection{GeneratedNoise Dataset}
\label{subsubsec:generatednoise_intro}

This dataset was generated by using pytorch's random tensor function
\codeword{torch.rand()}, which generates a tensor of specified dimensions,
where each entry is a float uniformly distributed within $(0, 1)$. To make this
data a little easier to view, this data was then smoothed using
\codeword{scipy.ndimage.filters.gaussian_filter1d()} function, which convolves
the signal with a 1-dimensional gaussian kernel with $\sigma = 1$. The train
split consists of 22000 samples, and the test set consists of 7000.

\subsubsection{Stocks Dataset}
\label{subsubsec:stocks_intro}

This dataset was sourced using Alpha Vantage \cite{alpha_vantage}, a free
online stock price API. Close price data is provided from the past two years
for the NASDAQ tickers `GOOGL', `MSFT', `TSLA', `AAPL', `AMZN', `NVDA', `FB',
and `AMD'. This data is normalized by subtracting the minimum and dividing by
the maximum for each sequence individually, such that each sequence pulled from
the dataset lies within $(0, 1)$, and is therefore comparable to the other
datasets. The train split consists of 22277 samples, and the test set consists
of 7312 samples.

\subsection{Sequence Prediction}
\label{subsec:experiment_sp}

The MSE loss was recorded as the model was trained, and a plot of this is shown
in Figure \ref{plt:lstm_train_loss}. In addition, after training was complete,
a plot was generated of the model's average loss on the test split as the
predicted sequence progresses, and it is shown in Figure
\ref{plt:lstm_seq_loss}. The average loss over the whole test split is shown in
Table \ref{tab:lstm_avg_loss}.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{plots/lstm_train_loss.png}
	\end{center}
	\caption{LSTM Training Loss on GeneratedSins, GeneratedNoise, and Stocks
	Datasets for Models with Varying Number of Cell Layers}
	\label{plt:lstm_train_loss}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{plots/lstm_seq_loss.png}
	\end{center}
	\caption{LSTM Test Loss over Predicted Sequence Steps for GeneratedSins, GeneratedNoise, and Stocks Datasets for Models 
	with Varying Number of Cell Layers}
	\label{plt:lstm_seq_loss}
\end{figure}

\begin{table}[H]
	\caption{LSTM Average Test Losses vs. Number of Layers}
	\label{tab:lstm_avg_loss}
	\begin{center}
		\begin{tabular}[c]{ p{0.7in} p{1.3in} p{1.3in} p{1in} }
			\textbf{Layers} & \textbf{GeneratedNoise} & \textbf{GeneratedSins} & \textbf{Stocks} \vspace{0.3em} \\
			\textbf{1}      & \num{5.964e-2}          & \num{4.950e-4}         & \num{5.227e-2}                 \\
			\textbf{2}      & \num{5.967e-2}          & \num{6.298e-4}         & \num{5.431e-2}                 \\
			\textbf{3}      & \num{5.961e-2}          & \num{4.804e-4}         & \num{5.381e-2}
		\end{tabular}
	\end{center}
\end{table}


\subsubsection{GeneratedSins Experiment}
\label{subsubsec:generated_sins}

The results shown in the previous figures confirm this dataset to be the most
predictable of the three. This can be observed in the plot of training losses
in Figure \ref{plt:lstm_seq_loss}, which shows that models trained on
GeneratedSins data generally decreased the most over training out of the three,
and in the plot of average test losses over the predicted sequence in Figure
\ref{plt:lstm_train_loss}, which shows that GeneratedSins led to the most
accurate predictions over longer sequences by far. Additionally, Table
\ref{tab:lstm_avg_loss} shows that the average test loss from this dataset was
only \num{e-2} times the average losses of the others. A Test inference of a
2-layer LSTM model trained on this dataset with the ground truth sequence
underlaid is shown in Figure \ref{inf:lstm_sins_inference}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{inferences/sins/3layer_s11.png}
	\caption{4 LSTM Inferences on Test Split of GeneratedSins Dataset}
	\label{inf:lstm_sins_inference}
\end{figure}

As clean and impressive as these predictions are, however, they are not
perfect, and still decrease in accuracy as the sequence progresses. While the
errors are minor in this demonstration, the shortcoming reveals a pretty major
property of the Seq2Seq architecture (and all sequence prediction
architectures, not just Seq2Seq), which is that in general, predictions will
tend to decrease in accuracy as they become longer. In most situations, this is
because real data is at least some part stochastic, meaning that, as time goes
on, a sequence can have many possible outcomes, and the state of the sequences
at any far-off point in time is a composition of all the previous random events
that take place. In this situation, however, a feature of the Seq2Seq modeling
is most likely the culprit (excluding possible training and implementation
errors), since there is \textit{no} inherent stochasticity of a sin wave, that
is, it is not a random process at all and once it is established at any point
in time it will continue in that way forever. 

In this situation, when the model feeds each LSTM cell's output directly back
into itself as an input for the next time step in order to generate the full
sequence (See Figure \ref{fig:seq2seq_arch}), if any prediction is off by a
tiny amount, then the next prediction will likely share and even increase that
error. While it is likely that this is the phenomenon causing the majority of
the error in this situation, the small initial errors are most likely due to
incomplete convergence of the model, since the training and test set should
theoretically approach the same distribution as more samples are drawn\dots

\subsubsection{GeneratedNoise Experiment}
\label{subsubsec:generated_noise}

This dataset, in contrast, was nearly impossible for the model to learn
satisfactorily, as evidenced by the training loss plot shown in Figure
\ref{plt:lstm_train_loss} which is essentially flat. Additionally, over the
test sequences on average in Figure \ref{plt:lstm_seq_loss}, the model
increases most rapidly in error as the prediction increases in duration. While
this result is somewhat expected for this dataset, it is still concerning that
the model is capable of predicting any amount of a random signal for any
duration. The answer to this question lies in the details of the dataset
implementation, specifically that the points are not purely random; they become
slightly cross-correlated after being smoothed by the gaussian filter, due to
the convolution action which sums neighboring points. A batch of test
inferences showcasing this phenomenon is shown in Figure
\ref{inf:lstm_noise_inference}.

As for the model implementation and architecture, one important aspect of the
results on this dataset stands out, which is that the model predicts nothing
but a flat line centered at 0.5 after these initial predictions. This may seem
like an uninteresting result, but what it shows is fundamentally the same issue
that will cause blurry predictions in actual video predictions, and this is one
of the current challenges discussed in video prediction research
\cite{video_prediction_survey}.

To analyze the issue here, it is necessary to closely evaluate the loss
function used in training this model:

\begin{equation}
	\text{MSE} = \frac{1}{n} \sum_{i = 1}^n ( Y_i - \hat{Y}_i )^2
	 \label{eq:mseloss}
\end{equation}

The issue with this loss function is that it assumes that the underlying data
distribution is Gaussian. Here, it is nearly the case that $X, Y \sim U(0, 1)$,
and the expected value of the uniform distribution $\mu = \frac{1}{2} (1 - 0) =
0.5$ will minimize the MSE loss, even if $\mu$ itself has very low probability.
This feature of pixel-wise loss functions has negative side-effects for video
prediction, since the mean prediction does not usually yield very realistic
frames, and these types of predictions are not as directly useful to humans.
 
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{inferences/noise/3layer_s13.png}
	\caption{LSTM Inference on GeneratedNoise Dataset, with Ground Truth Input X in Blue, Ground Truth Label Y in Green, and Model Prediction in Purple}
	\label{inf:lstm_noise_inference}
\end{figure}

% The methods used to limit this negative effect are to use adversarial training,
% more complex loss functions, or modeling the prediction using probability \cite{}.

\subsubsection{Stocks Experiment}
\label{subsubsec:stocks}

Prior to performing this experiment, it was difficult to hypothesize how the
LSTM model would perform on the stock price dataset, partly because the author
knows very little about finance in general, but also because if it could be
done reliably, then it would not be hard to make a lot of money very easily!
However, from the previous experimentation with the GeneratedNoise dataset, it
was reasonable that stock data points should be more cross-correlated between
neighboring points, since stock prices should have some sort of momentum, that
is, if they are rising they should be slightly more likely to continue to rise,
and vice versa. 

In practice, the results of testing the model on this dataset showed some
interesting phenomenon and differences when compared to GeneratedNoise.
Firstly, the model does not just predict the mean for every sequence with this
dataset, instead, it attempts to fit each line individually, as can be seen in
Figure \ref{inf:lstm_stocks_inference}. In addition, the average test loss over
this dataset shown in Figure \ref{plt:lstm_seq_loss} reveals that the
prediction error does not decrease as quickly as with GeneratedNoise, and the
training loss plot shown in Figure \ref{plt:lstm_train_loss} shows that the
overall decrease in loss over the stocks dataset was greater than for
GeneratedNoise. These results are therefore very interesting, as they show that
the model does learn something from the stocks sequences, even though they are,
at first glance, just as random and unpredictable as the random noise.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{inferences/stocks/3layer_s31.png}
	\caption{LSTM Inference on Stocks Dataset, with Ground Truth Input X in Blue, Ground Truth Label Y in Green, and Model Prediction in Purple}
	\label{inf:lstm_stocks_inference}
\end{figure}

While some of the predictions from this dataset are impressive, however, and
seem to almost approximate the trajectory of the stock price, a good amount of
them are also mostly wrong, and even seem to diverge from the ground truth
label completely. From a qualitative perspective, these predictions do not seem
accurate enough to make any serious decisions off of, however, it would be very
interesting to test the performance of this model in making trading decisions
as a future experiment.

\newpage
\subsection{Video Prediction}
\label{subsec:experiment_vp}

Now on to the real meat of this project, which is to use a variation of the
previously-used model equipped with convolution in order to predict the
continuation of video sequences. 
 
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{plots/convlstm_train_loss.png}
	\caption{ConvLSTM Training Loss on MovingMNIST, KTH, and BAIR Datasets for Models with Varying Number of Cell Layers}
	\label{plt:convlstm_train_loss}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{plots/convlstm_seq_loss.png}
	\caption{ConvLSTM Test Loss over Predicted Sequence Steps on MovingMNIST, KTH, and BAIR Datasets for Models with Varying Number of Cell Layers}
	\label{plt:convlstm_seq_loss}
\end{figure}

As before, this model's training loss over the three video datasets
MovingMNIST, KTH, and BAIR are shown in Figure \ref{plt:convlstm_train_loss},
and the average test loss plotted over the prediction sequence steps for each
dataset and number of cell layers is shown in Figure
\ref{plt:convlstm_seq_loss}. Additionally, the average test loss is shown in
Table \ref{tab:convlstm_avg_loss}.

\begin{table}[H]
	\caption{ConvLSTM Average Test Losses For Each Dataset and Number of Layers}
	\label{tab:convlstm_avg_loss}
	\begin{center}
		\begin{tabular}[c]{ p{0.7in} p{1.3in} p{1in} p{1in} }
			\textbf{Layers} & \textbf{MovingMNIST} & \textbf{KTH}    & \textbf{BAIR} \vspace{0.3em} \\
			\textbf{1}      & \num{1.249e-02}      & \num{1.233e-03} & \num{2.910e-02}              \\
			\textbf{2}      & \num{1.278e-02}      & \num{1.239e-03} & \num{2.876e-02}              \\
			\textbf{3}      & \num{1.274e-02}      & \num{1.163e-03} & \num{3.019e-02}
		\end{tabular}
	\end{center}
\end{table}

\subsubsection{Moving MNIST}
\label{subsubsec:mmnist}

The model's inferences on the MovingMNIST were annoyingly mixed, with some
results being quite clear, such as the one shown in
\ref{inf:lstm_mmnist_inference_1}, and others being much more muddled and
blurry, such as the one shown in \ref{inf:lstm_mmnist_inference_2}. Since the
MovingMNIST dataset has elements of predictability (for example, digits bounce
off the walls in a predictable way and move at a constant velocity) the
expectation was that this dataset would be easy for the ConvLSTM model to
learn, however, the training loss plot in Figure \ref{plt:convlstm_train_loss}
and the average test sequence loss in Figure \ref{plt:convlstm_seq_loss} show
that this dataset was the most difficult of the three. 

As to why this is the case, it could be a combination of several factors: poor
bias learning, occlusion, or model depth and training time limitations. 

For the first reason, it is important to look at the ways in which MovingMNIST
is unique to the other datasets, for example, it is extremely bimodal, and
pixel intensity values densely cluster around 1 and 0, or white and black, as
they appear in images. Although the LSTM architecture (\ref{fig:lstmcell_arch})
includes a bias term in its weights, the bias needed here is not a simple
pixelwise correction, it is a correction term needed for each frame as the
digits bounce around. While a bias term such as the one included in the vanilla
LSTM cell implementation is capable of correcting the image predictions to be
mostly black, for example, it is not capable of learning that the pixels in
each digit as they bounce around should be white, since the bias terms are
constant over the entire prediction. Thus, the model struggles to produce such
black-and-white predictions.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{inferences/mmnist/mmnist_inferences_1.png}
	\end{center}
	\caption{Convolutional LSTM Inference on MovingMNIST dataset}
	\label{inf:lstm_mmnist_inference_1}
\end{figure}

While the second reason doesn't account for the high training loss directly,
since occlusion is only present in about half of the sequences and even then
isn't a major factor in most cases, it does increase the underlying complexity
of the dataset to some degree. This, combined with the third reason, could also
be major contributors to the poor prediction accuracy in this case, since the
model was only able to train over 10 epochs due to time and computing power
constraints. Additionally, the dataset is generated on the fly from the MNIST
digits dataset \cite{mnist_digits}, so it would probably help to train a deeper
model along with many more generated sequences.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{inferences/mmnist/mmnist_inferences_2.png}
	\end{center}
	\caption{Convolutional LSTM Inference on MovingMNIST dataset}
	\label{inf:lstm_mmnist_inference_2}
\end{figure}

\subsubsection{KTH}
\label{subsubsec:kth}

The ConvLSTM model trained and tested best on the KTH dataset out of the three,
as the loss plots in Figures \ref{plt:convlstm_train_loss} and
\ref{plt:convlstm_seq_loss} confirm, and this success led to some of the most
impressive and satisfying predictions in this project.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{inferences/kth/kth_inferences.png}
	\end{center}
	\caption{Convolutional LSTM Inference on KTH dataset (Sample 10)}
	\label{inf:lstm_kth_inference}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{inferences/kth/kth_inferences_2.png}
	\end{center}
	\caption{Convolutional LSTM Inference on KTH dataset (Sample 50)}
	\label{inf:lstm_kth_inference}
\end{figure}


\subsubsection{BAIR}
\label{subsubsec:bair}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1\textwidth]{inferences/bair/bair_inferences.png}
	\end{center}
	\caption{Convolutional LSTM Inference on BAIR dataset}
	\label{inf:lstm_bair_inference}
\end{figure}

\newpage
\section{Conclusion}
\label{sec:conclusion}

% ------------------------------------------------------------------------------
% References 
% ------------------------------------------------------------------------------

\newpage
\bibliography{main}
\newpage

\end{document}

